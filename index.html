<html>

<head>
    <meta charset="utf-8" />
    <title>Reference-based Super-Resolution via Image-based Retrieval-Augmented Generation Diffusion</title>

    <!-- Favicon references -->

    <meta
        content="Reference-based Super-Resolution via Image-based Retrieval-Augmented Generation Diffusion"
        name="description" />
    <meta
        content="Reference-based Super-Resolution via Image-based Retrieval-Augmented Generation Diffusion"
        property="og:title" />
    <meta
        content="Reference-based Super-Resolution via Image-based Retrieval-Augmented Generation Diffusion"
        property="og:description" />
    <meta
        content="Reference-based Super-Resolution via Image-based Retrieval-Augmented Generation Diffusion"
        property="twitter:title" />
    <meta
        content="Reference-based Super-Resolution via Image-based Retrieval-Augmented Generation Diffusion"
        property="twitter:description" />
    <meta property="og:type" content="website" />
    <meta content="summary_large_image" name="twitter:card" />
    <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"
        crossorigin="anonymous">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&family=Varela+Round&display=swap"
        rel="stylesheet">
    <link href="style.css" rel="stylesheet" type="text/css" />

    <!-- üîé Added minimal CSS for click‚Äëto‚Äëzoom lightbox -->
    <style>
      /* make images clearly zoomable */
      img.zoomable { cursor: zoom-in; transition: transform .2s ease; }

      /* fullscreen overlay */
      .lightbox-overlay {
        position: fixed; inset: 0; display: none; align-items: center; justify-content: center;
        background: rgba(0,0,0,.9); z-index: 10000;
      }
      .lightbox-overlay.active { display: flex; }
      .lightbox-overlay img { max-width: 95vw; max-height: 95vh; box-shadow: 0 10px 40px rgba(0,0,0,.6); border-radius: 8px; }
      /* show zoom-out cursor while overlay is open */
      .lightbox-overlay, .lightbox-overlay * { cursor: zoom-out; }

      /* prevent background scroll when overlay is open */
      body.no-scroll { overflow: hidden; }
    </style>

    <!-- MathJax for LaTeX rendering -->
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$','$$'], ['\\[','\\]']]
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            },
            svg: { fontCache: 'global' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" async></script>
</head>

<body>
    <!-- <header class="site-header">
        <div class="container">
            <nav class="main-nav">
                <ul class="nav-links">
                    <li><a href="index.html#overview">Overview</a></li>
                    <li><a href="index.html#motivation">Motivation</a></li>
                    <li><a href="index.html#experimental-results">Experiment</a></li>
                    <li><a href="index.html#conclusion">Conclusion</a></li>
                    <li><a href="index.html#citation">Citation</a></li>
                </ul>
            </nav>
        </div>
    </header> -->

    <div class="hero-section">
        <div class="container">
            <div class="title-row">
                <div class="title-flex">
                    <div class="title-text-block">
                        <!-- <h1 class="title"><span class="gradient-text"></span>Dual Recursive Feedback on Generation and Appearance Latents <span class="title-break"></span>  for Pose-Robust Text-to-Image Diffusion</h1> -->
                         <h1 class="title">
                            Reference-based Super-Resolution via Image-based Retrieval-Augmented Generation Diffusion
                        </h1>
                        <h1 class="subtitle"><span class="plum-text">ICCV 2025</span></h1>
                    </div>
                </div>
            </div>
            <div class="base-row author-row">
                <div class="base-col author-col">
                    <a href= "https://scholar.google.com/citations?user=0VhcJXwAAAAJ&hl" target ="_blank" class="author-text">
                        Byeonghun Lee<sup>1*</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://github.com/hyeon-cho" target="_blank" class="author-text">
                        Hyunmin Cho<sup>1*</sup></a>
                </div>
                <div class="base-col author-col">
                    <span class="author-text">
                        Hong Gyu Choi<sup>2</sup>
                    </span>
                </div>
                <div class="base-col author-col">
                    <span class="author-text">Soo Min Kang<sup>2</sup></span>
                </div>
                <div class="base-col author-col">
                    <span class="author-text">
                        Iljun Ahn<sup>2</sup>
                    </span>
                </div>
                <div class="base-col author-col">
                    <a href="https://scholar.google.com/citations?user=aLYNnyoAAAAJ&hl" target="_blank" class="author-text">
                        Kyong Hwan Jin<sup>1‚Ä†</sup>
                    </a>
                </div>
            </div>
            <div class="base-row author-row">
            <div class="base-col author-col affiliations">
                <div class="affil-line">
                <span class="affil-item"></span>
                <span class="affil-item"><sup>1</sup>Korea University</span>
                <span class="affil-item"><sup>2</sup>Independent Researcher</span>
                </div>
                <div class="affil-line">
                <span class="affil-item"></span>
                <span class="affil-item"><sup>*</sup>equal contribution</span>
                <span class="affil-item"><sup>‚Ä†</sup>corresponding author</span>
                </div>
            </div>
            </div>

            <!-- <div class="base-row author-row">
                <div class="base-col author-col affiliations">
                    <sup>1</sup>Korea University &nbsp;&nbsp; <sup>2</sup>Independent Researcher &nbsp;&nbsp;
                    <br>
                    <sup>*</sup>equal contribution &nbsp;&nbsp;
                    <sup>‚Ä†</sup>corresponding author
                </div>
            </div> -->
            <div class="link-labels base-row">
                <div class="base-col icon-col"><a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Lee_Reference-based_Super-Resolution_via_Image-based_Retrieval-Augmented_Generation_Diffusion_ICCV_2025_paper.pdf" target="_blank"
                        class="link-block">
                        <img src="./asset/arXiv.png" alt="Hugging Face Logo" style="height: 5.0em; vertical-align: middle;">
                        <strong class="link-labels-text">Paper</strong>
                    </a></div>
                
                <div class="base-col icon-col"><a href='https://huggingface.co/' class="link-block">
                         <span class="icon">
                       <img src="./asset/hf-logo.png" alt="Hugging Face Logo" style="height: 5.0em; vertical-align: middle;">
                  </span>
                        <strong class="link-labels-text">Hugging Face</strong>
                    </a></div>
                <div class="base-col icon-col"><a href='https://github.com/ByeonghunLee12/iRAG_page' class="link-block">
                         <span class="icon">
                       <img src="./asset/github-mark.svg" alt="Github Logo" style="height: 5.0em; vertical-align: middle;">
                  </span>
                        <strong class="link-labels-text">Code</strong>
                    </a></div>
                <div class="base-col icon-col"><a href="index.html#citation" class="link-block">
                        <img src="./asset/Scholar.png" alt="Hugging Face Logo" style="height: 5.0em; vertical-align: middle;">
                        <strong class="link-labels-text">Citation</strong>
                    </a></div>
            </div>

        </div>
    </div>

    <main class="main-content">

        <div class="container">
                <img id="teaser" src="./asset/qual.png" alt="A descriptive text for the image"style="width:200%; max-width:1080px; max-height:480px; height:auto; object-fit:contain; display:block; margin:0 auto;">

            <div id="abstract" class="base-row section">
                <h2>Abstract</h2>
                <p class="paragraph">
                    Most existing diffusion models have primarily utilized reference images for image-to-image translation rather than for super-resolution (SR). In SR-specific tasks, diffusion methods rely solely on low-resolution (LR) inputs, limiting their ability to leverage reference information. Prior reference-based diffusion SR methods have shown that incorporating appropriate references can significantly enhance reconstruction quality; however, identifying suitable references in real-world scenarios remains a critical challenge. Recently, Retrieval-Augmented Generation (RAG) has emerged as an effective framework that integrates retrieval-based and generation-based information from databases to enhance the accuracy and relevance of responses. Inspired by RAG, we propose an image-based RAG framework (iRAG) for realistic super-resolution, which employs a trainable hashing function to retrieve either real-world or generated references given an LR query. Retrieved patches are passed to a restoration module that generates high-fidelity super-resolved features, and a hallucination filtering mechanism is used to refine generated references from pre-trained diffusion models. Experimental results demonstrate that our approach not only resolves practical difficulties in reference selection but also delivers superior performance over existing diffusion and non-diffusion RefSR methods. 
                </p>
            </div>
            <!-- <h3>
                Conceptual Visualization of Dual Recursive Feedback
            </h3>

            <div class="video-container">
            <video
              class="main-video"
              src="./asset/DRF_concept_video.mp4"
              autoplay
              loop
              muted
              playsinline
              controls
              style="width:100%; max-width:980px; max-height:420px; height:auto; object-fit:contain; display:block; margin:0 auto;">
              Your browser does not support the video tag.
            </video>
            </div>
            
            <p class="paragraph"> Illustration of a diffusion-based generative model with latent feedback mechanisms (DRF) for controlling both appearance and generation latent in <strong>class-invariant text-to-image synthesis</strong>. The proposed method refines latent updates to achieve fine-grained control, improving results with desired structural and appearance attributes.
            </p> -->
            <div class="image-container" style="margin-top: 2rem;">
            <div class="image-content">
                <img
                src="./asset/flow_chart.png"
                alt="iRAG pipeline"
                class="img"
                style="width:100%; max-width:1280px; max-height:500px; height:auto; object-fit:contain; margin:0 auto;"
                >
            </div>
            <!-- <p class="image-caption">
                DRF iteratively updates by obtaining guided noises \(\epsilon_{\theta}^a\)
                and \(\epsilon_{\theta}^g\) through appearance and generation feedback.<br>
                The distillation function derived from these two noises is combined to
                update the generation latent.
            </p> -->
            </div>


            <section id="method" class="section">
              <h2>Method</h2>
                    <p class="paragraph">
In the context of image super-resolution (SR), an auxiliary high-resolution (HR) image, containing semantically or texturally similar information to the input low-resolution (LR) image, is often leveraged to guide the restoration of fine details and structural integrity. However, obtaining such a reference image for real-world datasets is challenging. Large datasets, such as ImageNet, typically consist of single-view images, making it difficult for direct use as reference patches. Furthermore, the process of curating a reference image from these extensive datasets is computationally onerous. To address this challenge, we propose a reference-based SR method that involves three key steps: (i) <strong>augmenting</strong> the existing dataset by enriching it with auxiliary HR images, (ii) <strong>retrieving</strong>  a relevant HR image from a large database to match the target LR image, and (iii) <strong>generating</strong> a high-quality HR image by integrating LR and reference features into a diffusion model.
                    </p>
              <!-- 1) ÏúÑÏ™Ω: Appearance / Generation feedback (1:1) -->
              <div class="two-col method-equal">
                <div class="col-left">
                  <h3>Reference Database</h3>
                  <p class="paragraph">
                    We split the DF2K-OST dataset such that 75% of the images form the training set for image restoration. The remaining 25% is used to build the reference database, which is further augmented with synthetic images. To enrich this reference database, we add synthetic images in a one-to-one ratio with the real patches. We trained an unsupervised hashing model on these patches using positive pairs generated via standard data augmentation. Synthetic images were generated using the diffusers pipeline and SDEdit. A hallucination threshold (average variance of 0.03) was applied; if exceeded, the generation was repeated up to 10 times, retaining the sample with the lowest variance.
                  </p>
                </div>
            
                <div class="col-right">
                <h3></h3>
                  <img
                    src="./asset/image.png"
                    alt="Reference Database"
                    style="width:100%; max-width:980px; max-height:420px;
                           height:auto; object-fit:contain; display:block; margin:50px auto 0;"
                  >
                </div>
              </div>
            
              <!-- 2) Í∞ÄÏö¥Îç∞: ÌååÏù¥ÌîÑÎùºÏù∏ Í∑∏Î¶º Ï†ÑÏ≤¥ Ìè≠ -->
              <!-- <div class="image-container" style="margin-top: 2rem;">
                <div class="image-content">
                  <img
                    src="./asset/pipeline.png"
                    alt="DRF pipeline"
                    class="img"
                    style="width:100%; max-width:980px; max-height:420px; height:auto; object-fit:contain; margin:0 auto;"
                  >
                </div>
                <p class="image-caption">
                  DRF iteratively updates by obtaining guided noises \(\epsilon_{\theta}^a\)
                  and \(\epsilon_{\theta}^g\) through appearance and generation feedback.<br>
                  The distillation function derived from these two noises is combined to
                  update the generation latent.
                </p>
              </div>
            
              <div class="two-col method-equal" style="margin-top: 2.5rem;">
                <div class="col-left">
                  <h3>Dual Recursive Feedback</h3>
                  <p class="paragraph">
                    Dual Recursive Feedback (DRF) couples the two feedback loops into a
                    single training-free guidance rule. At each sampling step, we run the
                    controllable T2I backbone with appearance and generation prompts to
                    obtain guided noises \(\epsilon_{\theta}^a\) and \(\epsilon_{\theta}^g\),
                    from which we form denoised latents \(z_{0|t}^a\) and \(z_{0|t}^g\).
                    DRF defines an appearance reconstruction loss that pulls
                    \(z_{0|t}^a\) toward the reference code \(z_0^a\), and a
                    generation-consistency loss that keeps \(z_{0|t}^g\) close to the
                    previous output \(z_{\mathrm{prev}}^g\), weighted by
                    \(w_{\text{iter}}^{(i)}\). The gradient of this combined DRF loss is
                    used to refine the shared noise update for \(z_t^g\), so that samples
                    are recursively attracted to an identity-preserving, pose-consistent
                    fixed point while remaining fully plug-and-play with the underlying
                    diffusion sampler.
                  </p>
                </div>
            
                <div class="col-right img">
                  <img
                    src="./asset/algorithm.png"
                    alt="Algorithm 1. Dual Recursive Feedback"
                    style="width:100%; max-width:980px; max-height:420px;
                           height:auto; object-fit:contain; display:block; margin:0 auto;"
                  >
                  <p class="image-caption">
                    <strong>Dual Recursive Feedback.</strong> Appearance feedback pulls the
                    appearance latent toward an identity-preserving fixed point,<br>
                    while generation feedback recursively aligns the fused latent with the
                    desired structure and text condition.
                  </p>
                </div> -->
              <!-- </div> -->
            </section>

            

            <section id="conclusion" class="section">
                <h2>Conclusion</h2>

                <p class="paragraph">
We proposed a novel image-based Retrieval Augmented Generation framework that combines latent diffusion models with an efficient hashing code vector strategy achieving robust reference matching and realistic reference-based SR. Operating in a compact latent space by short binary hash codes, our method addressed the challenges of reference selection and improves domain consistency between low-resolution inputs and high-resolution references. Experiments on real-world datasets demonstrate that our approach outperforms existing diffusion-based super-resolution methods and reference-based methods in terms of fidelity, perceptual quality, and computational efficiency.
                </p>
            </section>

            <div class="citation add-top-padding">
                <h1 id="citation">Citation</h1>
                <p> If you use this work or find it helpful, please consider citing: </p>
                <pre id="codecell0">
@InProceedings{lee2025irag,
    author    = {Lee, Byeonghun and Cho, Hyunmin and Choi, Hong Gyu and Kang, Soo Min and Ahn, Iljun and Jin, Kyong Hwan},
    title     = {Reference-based Super-Resolution via Image-based Retrieval-Augmented Generation Diffusion},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2025},
    pages     = {10764-10774}
}
                </pre>
            </div>
        </div>
    </main>

    <footer class="site-footer">
        <div class="container">
            <p class="credit">Credit: The design of this project page is inspired by previous academic project pages, such as <a href="https://llm-grounded-diffusion.github.io/" target="_blank">LLM-grounded Diffusion</a>, <a href="https://describe-anything.github.io/" target="_blank">Describe-anything</a>, <a href="https://cvlab-kaist.github.io/VIRAL/" target="_blank">VIRAL</a>, and <a href="https://cvlab-kaist.github.io/SpikeMatch/" target="_blank">SpikeMatch</a>.</p>
        </div>
    </footer>


    <script>
    function toggleMute(element) {
        const video = element.parentElement.querySelector('video');
        const icon = element.querySelector('i');
        const text = element.querySelector('.unmute-text');
        
        if (video.muted) {
            video.muted = false;
            icon.className = 'fa fa-volume-up';
            text.textContent = 'Mute';
        } else {
            video.muted = true;
            icon.className = 'fa fa-volume-off';
            text.textContent = 'Click to unmute';
        }
    }
    
    document.addEventListener('DOMContentLoaded', function() {
        const videos = document.querySelectorAll('video');
        videos.forEach(video => {
            video.addEventListener('play', function() {
                const overlay = this.parentElement.querySelector('.unmute-overlay');
                if (overlay) overlay.style.opacity = '0.8';
            });
            
            video.addEventListener('pause', function() {
                const overlay = this.parentElement.querySelector('.unmute-overlay');
                if (overlay) overlay.style.opacity = '0.8';
            });
        });

        // Initialize all slideshows
        document.querySelectorAll('.slideshow-container').forEach(container => {
            const slideshow = container.querySelector('.slideshow');
            const slides = slideshow.querySelectorAll('.slide');
            const prevButton = container.querySelector('.slideshow-nav.prev');
            const nextButton = container.querySelector('.slideshow-nav.next');
            const playPauseButton = container.querySelector('.play-pause');
            
            let currentSlide = 0;
            let autoplayInterval;
            let isPlaying = true;

            function showSlide(n) {
                slides.forEach(slide => slide.classList.remove('active'));
                currentSlide = (n + slides.length) % slides.length;
                slides[currentSlide].classList.add('active');
            }

            function changeSlide(n) {
                showSlide(currentSlide + n);
                resetAutoplay();
            }

            function togglePlayPause() {
                if (isPlaying) {
                    clearInterval(autoplayInterval);
                    playPauseButton.innerHTML = '<i class="fa fa-play"></i>';
                } else {
                    startAutoplay();
                    playPauseButton.innerHTML = '<i class="fa fa-pause"></i>';
                }
                isPlaying = !isPlaying;
            }

            function startAutoplay() {
                autoplayInterval = setInterval(() => {
                    showSlide(currentSlide + 1);
                }, 5000);
            }

            function resetAutoplay() {
                clearInterval(autoplayInterval);
                if (isPlaying) {
                    startAutoplay();
                }
            }

            // Initialize this slideshow
            showSlide(0);
            startAutoplay();

            // Add event listeners
            prevButton.addEventListener('click', () => changeSlide(-1));
            nextButton.addEventListener('click', () => changeSlide(1));
            playPauseButton.addEventListener('click', togglePlayPause);
        });

        // Handle main video play button
        const mainVideo = document.querySelector('.main-video');
        const playButton = document.querySelector('.play-button-overlay');
        
        if (mainVideo && playButton) {
            // Click play button to play video
            playButton.addEventListener('click', () => {
                mainVideo.play();
                mainVideo.classList.add('playing');
            });

            // Handle video play/pause events
            mainVideo.addEventListener('play', () => {
                mainVideo.classList.add('playing');
            });

            mainVideo.addEventListener('pause', () => {
                mainVideo.classList.remove('playing');
            });

            mainVideo.addEventListener('ended', () => {
                mainVideo.classList.remove('playing');
            });
        }

        // -----------------------------
        // üñºÔ∏è Click-to-zoom Lightbox
        // -----------------------------
        // Build overlay once
        const lightbox = document.createElement('div');
        lightbox.className = 'lightbox-overlay';
        lightbox.setAttribute('role', 'dialog');
        lightbox.setAttribute('aria-modal', 'true');
        lightbox.innerHTML = '<img alt="Expanded image">';
        document.body.appendChild(lightbox);
        const lightboxImg = lightbox.querySelector('img');

        function openLightbox(src, alt) {
            lightboxImg.src = src;
            lightboxImg.alt = alt || '';
            lightbox.classList.add('active');
            document.body.classList.add('no-scroll');
        }
        function closeLightbox() {
            lightbox.classList.remove('active');
            document.body.classList.remove('no-scroll');
            lightboxImg.src = '';
        }

        // Close on click anywhere or on Esc
        lightbox.addEventListener('click', closeLightbox);
        document.addEventListener('keydown', (e) => {
            if (e.key === 'Escape' && lightbox.classList.contains('active')) closeLightbox();
        });

        // Mark target images as zoomable and wire up click
        const zoomableImages = document.querySelectorAll('.image-container img, .slideshow img, .main-content img.img, .hero-section img.img');
        zoomableImages.forEach(img => {
            img.classList.add('zoomable');
            img.addEventListener('click', () => {
                // support optional high-res source via data-fullsrc
                const src = img.getAttribute('data-fullsrc') || img.currentSrc || img.src;
                openLightbox(src, img.alt);
            });
        });
    });
    </script>
</body>
</html> 
